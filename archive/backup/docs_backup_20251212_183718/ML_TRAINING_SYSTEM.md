# ü§ñ Sistema de Guardado Autom√°tico ML - Comment Suggestions
**Fecha**: 7 de Diciembre, 2025  
**Estado**: ‚úÖ Implementado y Funcionando
---
## üéØ Objetivo
Cada vez que Ollama genera sugerencias de comentarios, guardar autom√°ticamente:
- **Contexto completo**: T√≠tulo, descripci√≥n, comentarios, tipo, estado, prioridad
- **Sugerencias generadas**: Texto, tipo, confianza
- **Metadata**: Timestamp, modelo usado
**Para qu√©**: Crear un dataset de entrenamiento que permita entrenar un modelo ML propio en el futuro.
---
## üèóÔ∏è Arquitectura Implementada
### Componentes Nuevos
#### 1. `api/ml_training_db.py` - Base de Datos ML
```python
class MLTrainingDatabase:
    """Almacena contextos y sugerencias para entrenamiento ML"""
    def add_training_sample(
        ticket_key, ticket_summary, ticket_description,
        issue_type, status, priority, all_comments,
        suggestions, model="ollama"
    ):
        # Genera hash √∫nico para evitar duplicados
        # Guarda contexto completo + sugerencias generadas
        # Auto-comprime a GZIP despu√©s de 100 muestras
```
**Caracter√≠sticas**:
- ‚úÖ **Detecci√≥n de duplicados**: Hash MD5 del contexto
- ‚úÖ **Compresi√≥n autom√°tica**: GZIP despu√©s de 100 muestras
- ‚úÖ **Estad√≠sticas detalladas**: Por tipo, estado, promedios
- ‚úÖ **Exportaci√≥n ML**: Formato listo para entrenamiento
#### 2. Integraci√≥n en `ml_comment_suggestions.py`
```python
def get_suggestions(...):
    # ... genera sugerencias con Ollama ...
    # NUEVO: Guardado autom√°tico
    if final_suggestions:
        ml_db = get_ml_training_db()
        ml_db.add_training_sample(
            ticket_key=ticket_key,
            ticket_summary=ticket_summary,
            ticket_description=ticket_description,
            issue_type=issue_type,
            status=status,
            priority=priority,
            all_comments=all_comments,
            suggestions=final_suggestions,
            model="ollama-llama3.2"
        )
```
**Flujo**:
1. Usuario solicita sugerencias
2. Ollama genera respuestas
3. Sistema guarda autom√°ticamente en DB ML
4. No bloquea respuesta al usuario (async)
#### 3. Nuevos Endpoints API
**GET `/api/ml/comments/ml-stats`** - Estad√≠sticas
```json
{
  "success": true,
  "stats": {
    "total_samples": 2,
    "total_suggestions": 4,
    "total_comments": 5,
    "avg_suggestions_per_sample": 2.0,
    "avg_comments_per_sample": 2.5,
    "by_issue_type": {
      "Bug": 1,
      "Performance": 1
    },
    "by_status": {
      "Open": 1,
      "In Progress": 1
    },
    "compressed": false,
    "created": "2025-12-07T23:58:43.823087",
    "last_modified": "2025-12-08T00:00:29.542115"
  }
}
```
**POST `/api/ml/comments/export-training-data`** - Exportar Dataset
```json
{
  "success": true,
  "message": "Training data exported successfully",
  "path": "data/ml_models/training_dataset.json",
  "samples": 2
}
```
---
## üìä Estructura de Datos
### Formato de Almacenamiento Interno
```json
{
  "training_samples": [
    {
      "context_hash": "a1b2c3d4e5f6...",
      "ticket_key": "PROJ-123",
      "timestamp": "2025-12-07T23:58:43.823087",
      "input": {
        "summary": "Error 404 en p√°gina principal",
        "description": "Los usuarios reportan error 404",
        "issue_type": "Bug",
        "status": "Open",
        "priority": "Critical",
        "comments": [
          "Iniciando investigaci√≥n",
          "Revisar configuraci√≥n del servidor"
        ],
        "comments_count": 2
      },
      "output": {
        "suggestions": [
          {
            "text": "La p√°gina principal se encuentra...",
            "type": "resolution",
            "confidence": 0.98
          }
        ],
        "suggestions_count": 3,
        "model": "ollama-llama3.2"
      }
    }
  ],
  "metadata": {
    "created": "2025-12-07T23:58:43.823087",
    "last_modified": "2025-12-08T00:00:29.542115",
    "total_samples": 2,
    "compressed": false,
    "version": "1.0"
  }
}
```
### Formato de Exportaci√≥n para ML
```json
[
  {
    "input": "Error 404 en p√°gina principal Los usuarios reportan error 404 Iniciando investigaci√≥n Revisar configuraci√≥n",
    "metadata": {
      "issue_type": "Bug",
      "status": "Open",
      "priority": "Critical"
    },
    "output_text": "La p√°gina principal se encuentra en estado de mantenimiento...",
    "output_type": "resolution",
    "confidence": 0.98
  }
]
```
**Caracter√≠sticas del formato exportado**:
- ‚úÖ **Input concatenado**: Summary + Description + Last 10 Comments
- ‚úÖ **Metadata separada**: Issue type, status, priority
- ‚úÖ **Output etiquetado**: Texto, tipo, confianza
- ‚úÖ **Listo para fine-tuning**: Compatible con frameworks ML
---
## üîÑ Flujo Completo
### 1. Usuario solicita sugerencias
```
Frontend ‚Üí POST /api/ml/comments/suggestions
```
### 2. Backend genera con Ollama
```python
# ml_comment_suggestions.py
suggestions = ollama_engine._call_ollama(prompt)
# ‚Üí [{"text": "...", "type": "diagnostic", "confidence": 0.95}, ...]
```
### 3. Guardado autom√°tico
```python
# AUTOM√ÅTICO, no requiere acci√≥n del usuario
ml_db.add_training_sample(
    ticket_key="PROJ-123",
    # ... contexto completo ...
    suggestions=suggestions,
    model="ollama-llama3.2"
)
```
### 4. Verificaci√≥n de duplicados
```python
context_hash = md5(f"{summary}|{description}|{comments}")
if context_hash in existing_samples:
    return  # Skip duplicate
```
### 5. Auto-compresi√≥n
```python
if len(samples) >= 100:
    save_compressed_gzip()
```
---
## üìà M√©tricas y Estad√≠sticas
### Estad√≠sticas Disponibles
```python
stats = ml_db.get_stats()
```
**Retorna**:
- `total_samples`: Total de contextos √∫nicos guardados
- `total_suggestions`: Total de sugerencias generadas
- `total_comments`: Total de comentarios analizados
- `avg_suggestions_per_sample`: Promedio de sugerencias por ticket
- `avg_comments_per_sample`: Promedio de comentarios por ticket
- `by_issue_type`: Distribuci√≥n por tipo de issue
- `by_status`: Distribuci√≥n por estado
- `compressed`: Si est√° usando compresi√≥n GZIP
- `created`: Fecha de creaci√≥n de la DB
- `last_modified`: √öltima modificaci√≥n
### Ejemplo Real
```json
{
  "total_samples": 2,
  "total_suggestions": 4,
  "total_comments": 5,
  "avg_suggestions_per_sample": 2.0,
  "avg_comments_per_sample": 2.5,
  "by_issue_type": {
    "Bug": 1,
    "Performance": 1
  },
  "by_status": {
    "Open": 1,
    "In Progress": 1
  },
  "compressed": false
}
```
---
## üéì Uso del Dataset para Entrenamiento ML
### Exportar Datos
```bash
curl -X POST http://127.0.0.1:5005/api/ml/comments/export-training-data
```
**Resultado**: `data/ml_models/training_dataset.json`
### Entrenar Modelo Propio
#### Opci√≥n 1: Fine-tuning de Transformer (BERT, RoBERTa)
```python
from transformers import AutoModelForSequenceClassification, Trainer
# Load dataset
with open('data/ml_models/training_dataset.json') as f:
    data = json.load(f)
# Prepare for Hugging Face
train_dataset = Dataset.from_dict({
    'text': [d['input'] for d in data],
    'label': [d['output_type'] for d in data]
})
# Fine-tune
model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')
trainer = Trainer(model=model, train_dataset=train_dataset)
trainer.train()
```
#### Opci√≥n 2: Fine-tuning de GPT-2/LLaMA
```python
# Para generaci√≥n de texto (output_text)
from transformers import GPT2LMHeadModel, Trainer
train_data = [
    f"Input: {d['input']}\nOutput: {d['output_text']}"
    for d in data
]
# Fine-tune GPT-2 en espa√±ol
model = GPT2LMHeadModel.from_pretrained('gpt2-spanish')
trainer.train()
```
#### Opci√≥n 3: Clasificador Simple (scikit-learn)
```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
# Vectorizar inputs
vectorizer = TfidfVectorizer(max_features=500)
X = vectorizer.fit_transform([d['input'] for d in data])
y = [d['output_type'] for d in data]
# Entrenar clasificador
clf = RandomForestClassifier()
clf.fit(X, y)
# Predecir tipo de sugerencia
prediction = clf.predict(vectorizer.transform(['Error en sistema...']))
```
---
## üöÄ Roadmap de Entrenamiento
### Fase 1: Colecci√≥n de Datos (ACTUAL)
- ‚úÖ **Sistema implementado**
- ‚úÖ Guardado autom√°tico
- ‚úÖ Detecci√≥n de duplicados
- ‚úÖ Compresi√≥n GZIP
- **Meta**: 500-1000 muestras
- **Tiempo estimado**: 2-4 semanas de uso normal
### Fase 2: An√°lisis y Limpieza
- Revisar distribuci√≥n de tipos
- Balancear dataset (igual cantidad de diagnostic/action/resolution)
- Eliminar sugerencias de baja calidad (confidence < 0.7)
- Validar consistencia de datos
### Fase 3: Entrenamiento de Modelo
- **Opci√≥n A**: Fine-tune BERT multiling√ºe para clasificaci√≥n
- **Opci√≥n B**: Fine-tune GPT-2 espa√±ol para generaci√≥n
- **Opci√≥n C**: Entrenar clasificador ligero (sklearn)
### Fase 4: Evaluaci√≥n
- Split 80/20 train/test
- M√©tricas: Accuracy, F1-score, Precision, Recall
- Comparar con Ollama baseline
- **Meta**: Accuracy > 85%
### Fase 5: Despliegue
- Integrar modelo entrenado en producci√≥n
- Sistema h√≠brido: Modelo propio + Ollama fallback
- Monitoring de performance
---
## üìÅ Estructura de Archivos
```
data/
‚îú‚îÄ‚îÄ cache/
‚îÇ   ‚îú‚îÄ‚îÄ ml_training_data.json          # DB sin comprimir (<100 muestras)
‚îÇ   ‚îî‚îÄ‚îÄ ml_training_data.json.gz       # DB comprimida (100+ muestras)
‚îî‚îÄ‚îÄ ml_models/
    ‚îî‚îÄ‚îÄ training_dataset.json          # Dataset exportado para ML
```
---
## üß™ Testing
### 1. Generar Muestras
```bash
# Generar sugerencia (guarda autom√°ticamente)
curl -X POST http://127.0.0.1:5005/api/ml/comments/suggestions \
  -H "Content-Type: application/json" \
  -d '{
    "summary": "Error en login",
    "description": "Usuarios no pueden acceder",
    "issue_type": "Bug",
    "status": "Open",
    "priority": "High",
    "all_comments": ["Revisando logs"],
    "max_suggestions": 3
  }'
```
### 2. Ver Estad√≠sticas
```bash
curl http://127.0.0.1:5005/api/ml/comments/ml-stats
```
### 3. Exportar Dataset
```bash
curl -X POST http://127.0.0.1:5005/api/ml/comments/export-training-data
```
### 4. Verificar Archivo
```bash
cat data/ml_models/training_dataset.json | jq '.[0]'
```
---
## üêõ Troubleshooting
### "Error saving to ML training DB"
```python
# Check logs
tail -f /tmp/speedyflow.log | grep "ML training"
```
### Dataset no crece
```python
# Verify hashing works
from api.ml_training_db import get_ml_training_db
ml_db = get_ml_training_db()
print(ml_db.get_stats())
```
### Duplicados no se detectan
```python
# Check context hash
import hashlib
context = f"{summary}|{description}|{'|'.join(comments)}"
hash_value = hashlib.md5(context.encode()).hexdigest()
print(f"Hash: {hash_value}")
```
---
## ‚úÖ Verificaci√≥n de Funcionamiento
**Prueba realizada**:
```bash
# 1. Gener√© sugerencia para "Error 404"
# 2. Gener√© la misma sugerencia (duplicado)
# 3. Gener√© sugerencia para "Sistema lento"
# Resultado:
# - total_samples: 2 (duplicado omitido) ‚úÖ
# - by_issue_type: Bug: 1, Performance: 1 ‚úÖ
# - avg_suggestions_per_sample: 2.0 ‚úÖ
```
---
## üìä Estado Actual
**Base de Datos ML**:
- ‚úÖ Implementada y funcionando
- ‚úÖ Guardado autom√°tico activo
- ‚úÖ Detecci√≥n de duplicados operativa
- ‚úÖ Compresi√≥n GZIP configurada (100+ muestras)
- ‚úÖ Endpoints de estad√≠sticas y exportaci√≥n funcionando
**Muestras Actuales**: 2 (reci√©n iniciado)
**Pr√≥ximo Paso**: Usar la aplicaci√≥n normalmente para acumular 500-1000 muestras
---
**Servidor**: http://127.0.0.1:5005  
**Ollama**: ‚úÖ Auto-iniciado con modelo llama3.2:latest  
**ML Training DB**: ‚úÖ Guardando autom√°ticamente  
**√öltima actualizaci√≥n**: 8 de Diciembre, 2025 00:00 UTC
