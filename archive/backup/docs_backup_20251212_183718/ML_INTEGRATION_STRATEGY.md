# ğŸš€ Estrategia de IntegraciÃ³n ML en SPEEDYFLOW MVP
## ğŸ“Š Estado Actual (6 Modelos Listos)
| Modelo | Accuracy | TamaÃ±o | Estado |
|--------|----------|--------|--------|
| Detector Duplicados | 90.12% | 0.57 MB | âœ… |
| Clasificador Prioridad | 99.64% | 0.57 MB | âœ… |
| Predictor SLA Breach | 85.29% | 0.59 MB | âœ… |
| Assignee Suggester | 23.41% | 1.42 MB | âœ… |
| Labels Suggester | 25% (P:91.67%) | 1.32 MB | âœ… |
| **Status Suggester** | **89.28%** | **0.58 MB** | âœ… |
**Total**: ~5 MB de modelos + 300 MB spaCy
---
## ğŸ—ï¸ Arquitectura Recomendada: MICROSERVICIO ML
### OpciÃ³n 1: **Servicio ML Independiente** (RECOMENDADO â­)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           SPEEDYFLOW MVP                    â”‚
â”‚  (Flask + HTML/CSS/JS)                      â”‚
â”‚                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚     Frontend Kanban Board            â”‚  â”‚
â”‚  â”‚   (HTML + Vanilla JS + Fetch API)    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚               â”‚                             â”‚
â”‚               â”‚ HTTP/REST                   â”‚
â”‚               â†“                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚    Backend API (Flask)               â”‚  â”‚
â”‚  â”‚  /api/issues, /api/transitions       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                  â”‚                â”‚
â”‚         â”‚ HTTP             â”‚ HTTP           â”‚
â”‚         â†“                  â†“                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ JIRA API    â”‚    â”‚  ML Service      â”‚  â”‚
â”‚  â”‚ (External)  â”‚    â”‚  Port 5001       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   ML Microservice (FastAPI)     â”‚
            â”‚   Port: 5001                    â”‚
            â”‚                                 â”‚
            â”‚  Endpoints:                     â”‚
            â”‚  â€¢ POST /ml/predict/duplicate   â”‚
            â”‚  â€¢ POST /ml/predict/priority    â”‚
            â”‚  â€¢ POST /ml/predict/sla-breach  â”‚
            â”‚  â€¢ POST /ml/suggest/assignee    â”‚
            â”‚  â€¢ POST /ml/suggest/labels      â”‚
            â”‚  â€¢ POST /ml/suggest/status      â”‚
            â”‚  â€¢ POST /ml/predict/all         â”‚
            â”‚                                 â”‚
            â”‚  Models (cargados en memoria):  â”‚
            â”‚  â€¢ 6 modelos Keras (~5MB)       â”‚
            â”‚  â€¢ spaCy es_core_news_md        â”‚
            â”‚  â€¢ Encoders/Binarizers          â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
### OpciÃ³n 2: **IntegraciÃ³n Directa en Flask** (MÃ¡s Simple)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      SPEEDYFLOW MVP (Flask)            â”‚
â”‚                                        â”‚
â”‚  Frontend â†’ Flask Routes â†’ ML Lib     â”‚
â”‚                      â†“                 â”‚
â”‚              SpeedyflowMLPredictor     â”‚
â”‚              (cargado al iniciar)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
---
## âš¡ ComparaciÃ³n de Opciones
| Aspecto | Microservicio ML | IntegraciÃ³n Directa |
|---------|-----------------|---------------------|
| **Escalabilidad** | â­â­â­â­â­ Escala independiente | â­â­ Limitada al proceso Flask |
| **Performance** | â­â­â­â­ HTTP overhead mÃ­nimo | â­â­â­â­â­ Sin overhead |
| **Mantenimiento** | â­â­â­â­â­ Aislado, fÃ¡cil update | â­â­â­ Acoplado |
| **Memoria** | â­â­â­â­â­ Proceso separado | â­â­ +305MB en Flask |
| **Deployment** | â­â­â­ 2 servicios | â­â­â­â­â­ 1 servicio |
| **Debugging** | â­â­â­â­ Logs separados | â­â­â­ Logs mezclados |
| **Caching** | â­â­â­â­â­ FÃ¡cil implementar | â­â­â­ Complejo |
| **Latencia** | ~10-50ms HTTP | <1ms local |
---
## ğŸ¯ RecomendaciÃ³n: MICROSERVICIO ML
### Por quÃ©?
1. **Memoria**: spaCy + modelos = 305MB â†’ No afectar Flask
2. **Escalabilidad**: Horizontal scaling independiente
3. **Desarrollo**: Equipo ML trabaja aislado
4. **ProducciÃ³n**: Restart ML sin afectar frontend
5. **CachÃ©**: Redis/Memcached fÃ¡cil de agregar
---
## ğŸ“¦ Estructura de Archivos Propuesta
```
SPEEDYFLOW-JIRA-Platform/
â”œâ”€â”€ api/                          # Flask Backend (Puerto 5000)
â”‚   â”œâ”€â”€ server.py
â”‚   â”œâ”€â”€ blueprints/
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ /                   # â­ NUEVO: Microservicio ML (Puerto 5001)
â”‚   â”œâ”€â”€ main.py                   # FastAPI app
â”‚   â”œâ”€â”€ predictor.py              # SpeedyflowMLPredictor
â”‚   â”œâ”€â”€ models/                   # Modelos entrenados
â”‚   â”‚   â”œâ”€â”€ duplicate_detector.keras
â”‚   â”‚   â”œâ”€â”€ priority_classifier.keras
â”‚   â”‚   â”œâ”€â”€ breach_predictor.keras
â”‚   â”‚   â”œâ”€â”€ assignee_suggester.keras
â”‚   â”‚   â”œâ”€â”€ labels_suggester.keras
â”‚   â”‚   â”œâ”€â”€ status_suggester.keras
â”‚   â”‚   â””â”€â”€ *.pkl (encoders)
â”‚   â”œâ”€â”€ cache/                    # Cache de predicciones
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ static/
â”‚   â”‚   â””â”€â”€ js/
â”‚   â”‚       â””â”€â”€ ml_client.js      # â­ Cliente JS para ML API
â”‚   â””â”€â”€ templates/
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ ml_predictor.py           # Clase predictor (shared)
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ scripts/                      # Scripts de entrenamiento
â”‚   â”œâ”€â”€ train_*.py
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ docs/
    â””â”€â”€ ML_API.md                 # â­ DocumentaciÃ³n API ML
```
---
## ğŸ”Œ API Endpoints del Microservicio ML
### 1. Predict All (Recomendado para UI)
```http
POST /ml/predict/all
Content-Type: application/json
{
  "summary": "Error en API de autenticaciÃ³n",
  "description": "Usuarios no pueden hacer login..."
}
Response:
{
  "duplicate_check": {
    "is_duplicate": false,
    "confidence": 0.94,
    "similar_tickets": ["MSM-1234"]
  },
  "priority": {
    "suggested": "High",
    "confidence": 0.87
  },
  "sla_breach": {
    "will_breach": true,
    "risk_level": "HIGH",
    "probability": 0.73
  },
  "assignee": {
    "suggestions": [
      {"name": "carlos.quintero", "confidence": 0.45},
      {"name": "adrian.villegas", "confidence": 0.32}
    ]
  },
  "labels": {
    "suggested": ["backend", "api", "auth"],
    "confidence": [0.82, 0.75, 0.68]
  },
  "status": {
    "next_status": "En Progreso",
    "confidence": 0.89
  }
}
```
### 2. Predict Individual (MÃ¡s rÃ¡pido)
```http
POST /ml/predict/priority
POST /ml/suggest/assignee
POST /ml/suggest/status
...
```
### 3. Health Check
```http
GET /ml/health
Response:
{
  "status": "healthy",
  "models_loaded": 6,
  "memory_usage": "320MB",
  "uptime": "2h 15m"
}
```
---
## ğŸš€ Plan de ImplementaciÃ³n (3 Fases)
### Fase 1: Setup Microservicio (1 dÃ­a)
- [ ] Crear `/` con FastAPI
- [ ] Mover modelos a `/models/`
- [ ] Implementar endpoints bÃ¡sicos
- [ ] Docker + docker-compose
- [ ] Pruebas locales
### Fase 2: IntegraciÃ³n Frontend (1 dÃ­a)
- [ ] Cliente JS para ML API (`ml_client.js`)
- [ ] Integrar en formulario de creaciÃ³n
- [ ] Mostrar sugerencias en UI
- [ ] Alertas de duplicados/SLA
### Fase 3: OptimizaciÃ³n (1 dÃ­a)
- [ ] Cache con Redis
- [ ] Rate limiting
- [ ] Batch predictions
- [ ] Monitoring (Prometheus)
- [ ] Logs estructurados
---
## ğŸ’» CÃ³digo Base del Microservicio
### `/main.py` (FastAPI)
```python
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from predictor import SpeedyflowMLPredictor
import time
app = FastAPI(title="SPEEDYFLOW ML Service", version="1.0.0")
# CORS para frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5000"],
    allow_methods=["*"],
    allow_headers=["*"],
)
# Cargar modelos al iniciar
predictor = SpeedyflowMLPredictor(models_dir="./models")
class PredictRequest(BaseModel):
    summary: str
    description: str = ""
@app.post("/ml/predict/all")
async def predict_all(req: PredictRequest):
    start = time.time()
    predictions = predictor.predict_all(req.summary, req.description)
    elapsed = time.time() - start
    return {
        **predictions,
        "latency_ms": int(elapsed * 1000)
    }
@app.get("/ml/health")
async def health():
    return {
        "status": "healthy",
        "models_loaded": len(predictor.models)
    }
```
### `frontend/static/js/ml_client.js`
```javascript
class MLClient {
    constructor(baseURL = 'http://localhost:5001') {
        this.baseURL = baseURL;
    }
    async predictAll(summary, description) {
        const response = await fetch(`${this.baseURL}/ml/predict/all`, {
            method: 'POST',
            headers: {'Content-Type': 'application/json'},
            body: JSON.stringify({summary, description})
        });
        return response.json();
    }
    async checkDuplicate(summary, description) {
        const data = await this.predictAll(summary, description);
        return data.duplicate_check;
    }
}
const mlClient = new MLClient();
```
---
## ğŸ¨ UI Integration Examples
### 1. Auto-complete en CreaciÃ³n de Ticket
```javascript
// Al escribir summary
document.getElementById('summary').addEventListener('blur', async (e) => {
    const summary = e.target.value;
    const predictions = await mlClient.predictAll(summary, '');
    // Auto-rellenar prioridad
    if (predictions.priority.confidence > 0.8) {
        document.getElementById('priority').value = predictions.priority.suggested;
        showSuggestionBadge('Prioridad sugerida por IA');
    }
    // Sugerir asignados
    const assigneeSelect = document.getElementById('assignee');
    predictions.assignee.suggestions.slice(0, 3).forEach(a => {
        const option = new Option(`${a.name} (${(a.confidence*100).toFixed(0)}%)`, a.name);
        assigneeSelect.add(option);
    });
});
```
### 2. Alerta de Duplicados
```javascript
async function checkForDuplicates(summary, description) {
    const dup = await mlClient.checkDuplicate(summary, description);
    if (dup.is_duplicate && dup.confidence > 0.7) {
        showAlert({
            type: 'warning',
            title: 'âš ï¸ Posible ticket duplicado',
            message: `Similar a: ${dup.similar_tickets.join(', ')}`,
            buttons: ['Continuar', 'Ver similares']
        });
    }
}
```
### 3. Badge de Riesgo SLA
```javascript
async function showSLARisk(summary, description) {
    const sla = await mlClient.predictAll(summary, description).sla_breach;
    if (sla.risk_level === 'HIGH') {
        const badge = document.createElement('span');
        badge.className = 'badge badge-danger';
        badge.innerHTML = 'ğŸš¨ Alto riesgo de violar SLA';
        document.getElementById('ticket-header').appendChild(badge);
    }
}
```
---
## ğŸ“Š Performance Esperado
| OperaciÃ³n | Latencia | Throughput |
|-----------|----------|------------|
| Predict All | 15-30ms | 50-100 req/s |
| Single Model | 5-10ms | 200-500 req/s |
| Con Cache | 1-2ms | 1000+ req/s |
---
## ğŸ³ Docker Setup
### `/Dockerfile`
```dockerfile
FROM python:3.11-slim
WORKDIR /app
# Instalar dependencias
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
# Descargar spaCy model
RUN python -m spacy download es_core_news_md
# Copiar cÃ³digo
COPY . .
EXPOSE 5001
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "5001"]
```
### `docker-compose.yml`
```yaml
version: '3.8'
services:
  speedyflow:
    build: ./api
    ports:
      - "5000:5000"
    depends_on:
      - ml-service
  ml-service:
    build: ./
    ports:
      - "5001:5001"
    environment:
      - MODELS_DIR=/app/models
    volumes:
      - ./models:/app/models
```
---
## âœ… Ventajas Clave
1. **Zero Downtime**: Actualizar ML sin reiniciar Flask
2. **Escalabilidad**: Load balancer â†’ N instancias ML
3. **CachÃ© Inteligente**: Redis con TTL por tipo de predicciÃ³n
4. **Monitoring**: MÃ©tricas ML separadas de Flask
5. **Desarrollo**: Equipos trabajan en paralelo
6. **Testing**: Unit tests ML aislados
---
## ğŸ¯ Siguiente Paso
Â¿QuÃ© prefieres implementar primero?
**OpciÃ³n A**: Microservicio ML completo (FastAPI + Docker)
**OpciÃ³n B**: IntegraciÃ³n directa en Flask (mÃ¡s rÃ¡pido)
**OpciÃ³n C**: Primero crear cliente JS + mock API
Mi recomendaciÃ³n: **OpciÃ³n A** para un MVP profesional y escalable.
